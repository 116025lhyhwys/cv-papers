# Densely Connected Convolutional Networks



Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections—one between each layer and its subsequent
layer—our network has L(L+1) 2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet

​           最近的研究表明，如果卷积网络在靠近输入层与输出层之间的地方有**更短的连接**（shorter connections），就可以训练更深、更准确、更有效的卷积网络。本文中，我们拥抱这个观点，介绍了稠密卷积网络（DenseNet），该网络在前馈时将每一层都与其他的任一层进行了连接。传统的$L$ 层卷积网络有$L$个连接——每一层与后一层有一个连接——我们的网络有$L(L+1)/2$个连接。每一层都将之前的所有层的特征图作为输入，而它自己的特征图是之后所有层的输入。DenseNets有一些显著优点：缓解梯度消失问题，加强特征传播，鼓励特征的重复利用，还大大减少参数量。我们在四个目标识别任务(CIFAR-10，CIFAR-100，SVHN和ImageNet）中评估了我们提出了结构。DenseNets在大部分数据集上相对state-of-the-art有明显提高，而且使用更少的计算量就可以获得高性能。代码和预训练模型可以在https://github.com/liuzhuang13/DenseNet上获得。    